---
title: "Введение: Терминология искусственного интеллекта"
description: Этот глоссарий не охватывает все существующие термины, поскольку он предназначен только для ознакомительного обзора. Для получения более полной информации рекомендуется продолжить изучение самостоятельно.
date: 2025-11-18
category: Терминология
author: veilosophy
---

# Промптинг и инженерия запросов

## Промпт-инженерия
это процесс структурирования или составления запроса (промпта) к языковой модели для получения от неё наиболее точного и полезного ответа. Например, вместо простого «Расскажи о фотографиях», можно сказать «Опиши эту картинку с пляжем и закатом» – такой запрос чётко задаёт контекст.

## Zero-shot prompting (Нуль-шот)
подход, когда модель удаётся только само задание без примеров нужного вывода. Модель опирается на общие знания из предобучения, чтобы решить задачу сразу. Например, попросить классифицировать срочную задачу IT так: «Класс проблемы: Высокая, Средняя или Низкая», и модель самостоятельно поймёт контекст.

## One-shot prompting (Ван-шейп)
к модели добавляется ровно один пример задачи. Например: «Пример: Переведи фразу “Bonjour” с французского на английский – ответ “Hello”. Теперь переведи: “Au revoir”». Модель видит один пример и применяет правило к новой фразе.

## Few-shot prompting (Фью-шот)
когда в запросе даётся несколько примеров (обычно 2–10) задач с правильными ответами. Модель учится на этих образцах и затем решает новую задачу. Например, показав пару фраз с разметкой «Положительный/Отрицательный» тональности, потом просят классифицировать другую фразу.

## Chain-of-Thought (CoT)
техника «цепочки размышлений», при которой модель последовательно выводит промежуточные шаги решения перед окончательным ответом. Например, задавая вопрос «У столовой было 23 яблока, 20 съели, потом купили ещё 6, сколько стало всего?», модель сначала выведет «23-20=3; 3+6=9», а потом ответит «9».

## ReAct (Reasoning + Acting)
подход, сочетающий рассуждения и действия. Модель формулирует «мысли» (сокращённые Chain-of-Thought) и «действия» (например, запрашивает информацию из внешнего источника), затем снова размышляет. Например, для поиска информации модель может отвечать в формате «Вопрос – Мысль – Действие – Наблюдение».

## Self-Consistency
метод повышения надёжности вывода: модель многократно генерирует ответ на один и тот же вопрос (обычно с CoT), а затем берётся «мнение большинства» среди этих ответов. Например, если из 5 запусков классификации письма дважды вышло «IMPORTANT» и трижды «NOT IMPORTANT», итоговым считается «IMPORTANT» (2 из 5). Это снижает влияние случайных вариаций.

## Multimodal prompting
включение разных типов данных в запрос (например, изображения + текст). Модель учится учитывать визуальный контекст. Например, показывают фото вечеринки и дают текстовый промпт «Опиши это событие» – модель интегрирует признаки с картинки и текста, чтобы получить детальное описание.

## Prompt Chaining (цепочка промптов)
разбивка сложной задачи на последовательность связанных запросов. К примеру, чтобы обработать текст на испанском, можно задать цепочку: «1) Прочитай текст. 2) Переведи его на английский. 3) Извлеки основные факты. 4) Составь список пунктов. 5) Переведи список обратно на испанский». Каждый шаг уточняет задачу и передаёт свой результат дальше.

## Meta-Prompting (метапромптинг)
продвинутый приём, фокусирующийся не на конкретном контенте, а на структуре и синтаксисе задачи. Здесь в запросе задаются абстрактные шаблоны или правила («подумай шаг за шагом») независимо от содержания. Это помогает унифицировать работу модели: например, предлагать ей абстрактный алгоритм рассуждений, а не конкретные примеры.

## Dynamic Prompting (динамическое промптирование)
адаптация промптов «на лету» в зависимости от сложности задачи и промежуточных результатов модели. Идея в том, что модель может сама менять длину цепочек рассуждений или уточнять запрос, если первый ответ оказался неудачным. Например, если «думать шаг за шагом» мало, она может добавить дополнительные шаги. Эта самонастройка повышает устойчивость, особенно для небольших моделей.

## Retrieval-Augmented Generation (RAG)
гибрид LLM + поиск. Суть в том, что перед генерацией ответ моделирует ищет дополнительную информацию во внешних источниках и использует её при составлении ответа. Например, чат-бот сначала находит релевантные документы или статьи (веб-поиск или база знаний), затем генерирует ответ, учитывая найденный текст. Это улучшает точность и снижает «галлюцинации», поскольку модель опирается на конкретные данные.

## Self-Prompting (самопромптирование)
метод, при котором модель самостоятельно генерирует вспомогательные подсказки или примеры. Например, LLM может сначала сгенерировать вымышленную пару «вопрос–ответ» или короткий текст и задать себе на их основе дальнейшую задачу. Идея в том, чтобы модель сама создала демонстрационные примеры для обучения «на себе». Например, она может придумать небольшой информативный абзац и несколько вопросов к нему, а затем использовать эти пары для дальнейшего ответа на похожие запросы.

## Tree of Thought (TAP) Prompting (дерево мыслей)
расширение CoT, при котором модель строит сразу несколько параллельных линий рассуждений, «ветвей» мышления, и выбирает наиболее удачные. Это как рисовать дерево возможных решений вместо одной цепочки. Модель может «откатываться» и исследовать альтернативы, если первый путь не приводит к ясному решению.

## Guided Prompting
в запрос встраиваются конкретные указания или ограничения, чтобы направить модель к нужному результату. Например, вместо «Как составить бюджет?» дают «Пошагово расскажи, как создать личный бюджет на следующий год» – это побуждает модель выдать детальный план. По сути, задаётся более явное руководство («Guided = с направлением») в промпте.

## Mix of Experts (MoE)
архитектура с «смесью экспертов»: модель разбивается на несколько специализированных «экспертных» подсетей, и для каждого входа активируется лишь часть из них. Каждый эксперт учится на своей части данных. Например, в LLM-MoE есть множество FFN-блоков (экспертов) в слое, а «роутер» выбирает для текущего токена самые релевантные эксперты. Это позволяет масштабировать модель при сохранении эффективности.

## Adaptive Prompting (адаптивное промптирование)
метод, где промпты динамически изменяются на основе хода рассуждений модели. Модель может сама корректировать запросы, добавлять уточнения или переформулировать вопрос в процессе. Идея близка к Self-Consistency и Dynamic Prompting – модель «прислушивается» к себе и подстраивает промпт для лучшего результата. (Подходы описываются в современных исследованиях, например в статье о self-adaptive prompting.)

## System Prompting (системные подсказки)
специальные инструкции, задаваемые не пользователем, а системой/разработчиком, чтобы установить рамки поведения модели. Например, в ChatGPT можно задать системное сообщение «Ты – дружелюбный помощник, обучающий школьников»; модель станет отвечать в стиле учителя. Системный промпт фиксирует тон, стиль, ограничения («не задавать личных вопросов» и т.п.) на протяжении всей сессии.

## Role Prompting (ролевое промптирование)
разновидность системного промптинга, где модели приписывается конкретная роль или персона. Например, «Ты – эксперт по истории» или «ты – критик, пишущий обзор фильма». Модель затем будет отвечать в характере заданной роли. Это изменяет стиль, глубину и точность ответа под нужды задачи. В примере роли «рецензент» текст становится более подробным и «публицистическим».

## Context Length Management (управление длиной контекста)
учёт ограничения на число токенов, которые модель может обработать за запрос. У каждой LLM есть максимальное окно контекста (например, GPT‑4 — 8K токенов). При длинных входах практикуют сжатие или разбивку текста, чтобы не превысить лимит и сохранить ключевую информацию. Например, делят большой документ на части или дают краткий пересказ перед основной задачей. Управление контекстом гарантирует, что важный контент влезет в «память» модели.

## Few-shot Calibration
приём корректировки вывода в условиях few-shot. Например, добавлением «нулевых» примеров или выравниванием распределения меток, чтобы убрать предвзятость. Конкретные техники включают добавление «градуированных» примеров («Это пример положительного отзыва») для стабилизации модели. (Упоминается в исследованиях по настройке prompt-демонстраций, но явных источников мало.)

## Max Tokens (максимум токенов)
в API генеративных моделей параметр, ограничивающий длину ответа. Это максимально допустимое число токенов в выходе модели. Если сумма токенов запроса и ответа превысит лимит, выполнение оборвётся. Например, при max_tokens=500 и 3500 токенах в запросе (из общего лимита 4000) останутся лишь 500 «условных слов» на ответ. Этот параметр помогает контролировать объём генерируемого текста и затраты времени.
Обучение моделей и адаптация

## Pre-training (предобучение)
начальный этап обучения больших моделей. Модель загружает огромный объём разнородного текста (интернет, книги) и учится предсказывать следующий токен в предложении. В этой фазе она «читает» миллиарды слов, вырабатывая общее понимание языка (грамматику, факты, логику). Это похоже на то, что модель «читает тонны книг», чтобы выучить язык. Например, в процессе модель постоянно практикуется в заполнении пропусков в текстах и становится способной генерировать связные ответы.

## Fine-tuning (тонкая настройка)
дообучение уже предобученной модели на узкоспециализированном датасете. Здесь модель «подтягивается» под конкретную задачу (отраслевой текст, код, медицинские записи и т.д.). Например, берут GPT и обучают его на юридических текстах, чтобы он хорошо отвечал на правовые вопросы. В отличие от предобучения, fine-tuning требует заметно меньшего объёма данных, но приводит к тому, что модель выдаёт более целенаправленные ответы в своём домене.

## RLHF (Reinforcement Learning from Human Feedback)
обучение с подкреплением на основе человеческой обратной связи. Сначала собираются ответы модели и люди оценивают их (что звучит более «человечно», «вежливо», «точно» и т.д.). На основе этих оценок строится «reward-модель», которая превращает качества ответа в числовую награду. Затем саму генеративную модель дообучают, чтобы она максимизировала эту награду. Цель – заставить ИИ соответствовать человеческим предпочтениям. Например, при машинном переводе RLHF помогает выбирать не просто «правильный», но и «естественный» вариант перевода.

## LoRA (Low-Rank Adaptation)
эффективный метод адаптации больших моделей без полного переобучения. Вместо того, чтобы править все веса, к каждому слою добавляются небольшие «низкоранговые» матрицы, которые обучаются под новую задачу. Модель при этом остаётся «замороженной», а мы настраиваем только эти малые модули. Например, полный GPT-3 (175 млрд параметров) с помощью LoRA можно адаптировать, тренируя лишь 18 миллионов параметров – это существенно экономит ресурсы. После дообучения весовые матрицы LoRA можно «внедрить» в модель без потери качества.

## Adapter Layers (адаптер-слои)
аналогичная идея: между слоями предобученной сети вставляют маленькие тренируемые модули. Эти «адаптеры» учатся захватывать особенности новой задачи, при этом основной весовой «скелет» модели не меняется. Преимущество в том, что можно иметь один большой базовый LLM и подключать к нему разные адаптеры для каждой задачи. К примеру, BERT + адаптеры для классификации текста или генерации переводов. Адаптеры ускоряют дообучение и не требуют столько памяти, сколько полная тонкая настройка.

## Masked Language Modeling (MLM)
метод предобучения модели на задаче «заполнения масок». Из текста случайно «маскируют» (скрывают) некоторые слова, а модель учится предсказывать, какие слова были на их месте. Например, из «Я люблю ___ и мороженое» модель должна догадаться «шоколад». MLM часто используют в BERT-подобных моделях, чтобы они понимали весь контекст (слева и справа от слова) во время обучения.

## Causal Language Modeling (CLM)
обучение на предсказании следующего слова слева-на-право. Это подход, используемый в GPT-подобных моделях: модель читает текст от начала к концу и на каждом шаге прогнозирует следующий токен. При таком обучении модель видит только уже сгенерированную часть, не «заглядывая» вперёд. Именно такой режим позволяет GPT генерировать тексты последовательно: от первого слова до последнего.

## Next Sentence Prediction (NSP)
вспомогательная задача при предобучении BERT. Модель получает пару предложений и учится определять, является ли второе предложение «настоящим продолжением» первого или случайным. То есть проверяет, логически ли второе предложение следует за первым. Это тренирует модель захватывать более высокоуровневые связи между фразами.

## Contrastive Learning (контрастивное обучение)
метод представления данных, при котором модель учится по парам «похожих» и «непохожих» примеров. Цель – строить такое пространство признаков, где похожие объекты (картинки, тексты, аудио и т.д.) оказываются близко друг к другу, а непохожие – далеко. Например, CLIP обучался контрастивно: модель подтягивает представления изображения и соответствующей ему подписи ближе в пространстве, а несвязанные – раздвигает. В результате она «понимает» семантику изображений и текста.

## Autoencoders (автоэнкодеры)
нейронные сети-«кодировщики», обучающиеся восстанавливать входные данные. В простейшем варианте они сжимают вход («кодируют»), пропускают через узкое «бутылочное горлышко» и затем разворачивают («декодируют») обратно, стараясь получить исходный сигнал. Цель – найти «скрытые» переменные и наиболее важные признаки входных данных. Пример: автоэнкодер для картинок может научиться удалять шум или сжимать изображение до компактного вектора. В случае вариационных автоэнкодеров (VAE) этот подход расширяется на генерацию новых образцов.
Архитектуры и методы тонкой настройки

## PEFT (Parameter-Efficient Fine-Tuning)
общий термин для подходов (включая LoRA, адаптеры, prefix-tuning и др.), которые донастраивают лишь небольшую часть параметров модели, сохраняя остальную «замороженной». Это экономит вычислительные ресурсы.

## QLoRA (Quantized LoRA)
комбинированный подход: модель хранится в низкой точности (квантуется), а адаптация LoRA проводится с сохранением точности в критичных частях. Это позволяет ещё больше снизить требования к памяти при тонкой настройке.

## P-Tuning v2
расширение идеи prompt tuning. Векторные («мягкие») промпты добавляются не только в начале текста, но и на разных слоях модели. В результате такие встроенные «векторные подсказки» обучаются вместе с адаптацией и позволяют модели подстроиться под задачу.

## LoRA-fusion
метод объединения (слияния) нескольких LoRA-модулей для разных задач в одну модель. Позволяет переключаться между задачами без перекомпиляции всех весов, просто подгружая нужные LoRA-адаптеры.

## Delta Tuning
подход, где вместо полной настройки модели к новой задаче обучаются только разности (дельты) между исходными и новыми весами. Это аналогично LoRA/адаптерам: фиксируются основные веса и хранятся лишь поправки.

## Hypernetwork Adaptation
использование вспомогательной сети (гиперсети), которая генерирует или подстраивает веса основного LLM. Вместо прямой донастройки, гиперсеть создает обновления весов для конкретной задачи. Это обеспечивает гибкую адаптацию без изменения основного слоя.

## Prefix Tuning
метод, когда к каждому слою Transformer добавляются специальные «префикс-токены» (в виде обучаемых векторов). То есть модель получает на вход не только слова пользователя, но и этот «неявный» контекст, который настраивается. Таким образом достигается аналог fine-tuning: префиксы задают задачу модели на более глубоком уровне при минимальном изменении структуры.

## AdapterFusion
техника объединения нескольких адаптеров (Adapter Layers) для разных задач. Она обучает сеть, которая выбирает, как «смешать» выходы разных адаптеров перед декодером. Это позволяет использовать один LLM с несколькими адаптерами и на лету комбинировать их эффекты.

## Mixture-of-Experts Routing
механизм, где «роутер» выбирает, какие эксперты (подсети) из MoE активировать для каждого входа. Например, может выбирать 2–4 эксперта из доступных, основываясь на входном сигнале. Так достигается баланс между мощностью и эффективностью.

## Prompt-based distillation
метод «дистилляции» знаний модели через промпты: учитель-модель генерирует ответы на промпты, а ученическая модель обучается имитировать эти ответы. Это позволяет передавать знания больших моделей в облегченные версии.

## Model Parallelism (модель-параллелизм)
техника распределения частей одной модели по нескольким устройствам (GPU, TPU) для обучения или инференса. Например, один слой на одном GPU, другой – на другом. Это позволяет работать с очень большими моделями, не помещающимися в память одного устройства.

## Pipeline Parallelism (параллелизм конвейера)
разновидность распределения, когда разные слои модели обрабатывают данные последовательно по конвейеру на разных устройствах. Пока один кусок данных проходит через первый слой на устройстве A, другой проходит через второй слой на устройстве B и т.д. Это повышает пропускную способность обучения.

## Sparse Attention (разреженное внимание)
оптимизация механизма внимания: вместо «полного» внимания на все токены (O(n²)), внимание ограничивают локальными или структурированными областями. Например, модели Longformer или BigBird используют скользящее окно или случайные связи, чтобы обрабатывать длинные тексты эффективнее.

## FlashAttention
высокоэффективный алгоритм точного вычисления внимания, разработанный для GPU. Он использует оптимизированные тильнинговые операции, чтобы сократить число обращений к памяти и ускорить вычисления. Благодаря FlashAttention ускоряется обработка больших контекстов без потери точности.

# Мультиагентные системы и управление

## MCP (Multi-Agent Control Protocol)
протокол распределённого управления для групп агентов. Описывает, как несколько AI-агентов обмениваются информацией и координируют действия. (Под «MCP» иногда понимают и Model-Conditioned Prompting – технику в промптинге, когда структура подсказки зависит от модели.)

## Multi-Agent Reinforcement Learning (MARL)
обучение нескольких агентов в единой среде с учётом взаимодействия. Каждый агент учится принимать решения, учитывая возможные действия других. Применяется, например, в стратегиях или кооперативных играх.

## Self-Organizing Agents
агенты, способные самостоятельно формировать структуру взаимодействия без внешнего контроля. Например, роевые роботы, которые сами распределяют роли по сигналам окружения.
## Emergent Behavior (эмерджентное поведение)
появление в системе сложных коллективных эффектов из простых правил взаимодействия агентов. Например, в симуляции стай птиц каждый следует простым правилам: держаться вместе, не сталкиваться, избегать хищников – а в итоге получается стройная стая.

## Swarm Intelligence (интеллект роя)
подход, вдохновлённый коллективным поведением биологических групп (муравьёв, пчёл). Множество простых агентов (роботов, программ) координируются децентрализованно для решения задач (поиск, патрулирование), обычно через локальное взаимодействие и обмен «сигналами».

## Cooperative AI (кооперативный ИИ)
направление, изучающее, как обучать агентов эффективно сотрудничать друг с другом и с людьми. Включает разработку методов обучения, при которых агенты максимизируют общую полезность, а не только собственную. Цель – безопасные мультиагентные системы, где участники избегают конфликтов и обмана.

## Decentralized Agent Coordination (децентрализованная координация)
алгоритмы организации работы агентов без центрального лидера. Каждый агент действует автономно, обмен информацией и координация происходят по локальным правилам или протоколам (например, локальный обмен сообщениями, oracles).

## Agent-based Modeling (моделирование агентов)
метод симуляции сложных систем через набор автономных агентов. Позволяет изучать, как их взаимодействия на простом уровне ведут к глобальным эффектам (экономика, экология, социальные системы и т.д.).

## Sim2Real Transfer (сим-ту-реал)
технология переноса поведения агентов, обученных или протестированных в симуляции, в реальную среду. Для этого применяют методы адаптации, чтобы учесть разницу физики, шума сенсоров и т.п. Например, робот может учиться ходить в симуляторе, а затем применять полученные стратегии в реальной жизни (с доп. настройкой).

## Hierarchical Agent Architectures (иерархии агентов)
построение многослойных систем, где «высшие» агенты ставят задачи или управляют «низшими». Позволяет разбивать сложные задачи: например, верхний слой планирует цели, а нижний – выполняет движения. Иерархия улучшает масштабируемость и объяснимость поведения системы.

## Task Allocation Strategies (стратегии распределения задач)
методы, как распределять задачи между агентами. Может быть статическое распределение (руками прописано), или динамическое (агенты договариваются, кто что возьмёт по загрузке, способностям). Пример: в рое дронов каждый авсигнал о том, сколько энергии осталось, и перераспределяют маршруты для контроля территории.

## Inter-Agent Communication Protocols (протоколы общения агентов)
правила и форматы обмена сообщениями между агентами. Это могут быть формализованные языки запросов-ответов, сигнализации или общие «чаты» между роботами. Хороший протокол позволяет агентам быстро обмениваться нужной информацией (запросить помощь, сообщить о цели) и скоординироваться.

## Critic-Actor Agents (агенты «критик-актер»)
в RL это схема, где «актер» генерирует действия, а «критик» оценивает их и даёт сигнал обучения. В мультиагентной среде могут быть отдельные актёры и критики для каждой агенты или общий критик. Это упрощает обучение с подкреплением: актеры быстро учатся на основе «оценок» критика.

## Adversarial Agents (противоборствующие агенты)
агенты, обучающиеся играть «против» или обманывать других. Например, в играх «кто-то пытается победить», могут специально атаковать слабые стороны соперников. В обучении это часто используется для улучшения устойчивости: один агент учится хорошей стратегии, другой подбирает контрстратегии.

## Distributed Prompting (распределённое генерирование)
концепция, при которой несколько агентов совместно формируют запрос. К примеру, один агент генерирует идею, другой её уточняет, третий проверяет. Это похоже на «интеллект роя» в генерации текстов или решений: каждый добавляет часть информации.

## Autonomous Prompt Optimization (автономная оптимизация промптов)
метод, где агенты автоматически улучшают свои запросы к LLM. Например, агент может сам переформулировать или расширить промпт, основываясь на ответе, чтобы сделать его лучше. Такая итеративная оптимизация помогает эффективно использовать LLM без вмешательства человека.

# Визуальные и мультимодальные модели

## Diffusion Models (диффузионные модели)
класс генеративных моделей (например, Stable Diffusion), которые сначала зашумляют изображение до «шумового» состояния, а затем обучаются восстанавливать картинку из шума. Это альтернативный GAN-подход: вместо состязания с генератором модель учится в обратном направлении диффузии. Diffusion Models сейчас популярны в генерации высококачественных изображений.

## Score-based Generative Models
близки к диффузии: они используют градиенты «score-функции» распределения данных для постепенной генерации образцов. Такие модели учатся оценивать, насколько «немного» шум убавить на каждом шаге, чтобы получить реалистичный вывод.

## GANs (Generative Adversarial Networks)
класс сетей, где генератор учится создавать данные (например, картинки), а дискриминатор учится отличать реальные примеры от синтезированных. Они «сражаются» друг с другом: генератор улучшается, обманывая дискриминатор, а дискриминатор совершенствует «наблюдение». GAN-ы умеют генерировать очень реалистичные изображения и часто применяются в искусстве и медиа.

## VAEs (Variational Autoencoders)
автоэнкодеры особого вида, которые дополнительно моделируют распределение признаков. Они кодируют данные в параметризованное распределение (обычно нормальное) и затем семплируют оттуда при генерации. Это позволяет гладко интерполировать между образцами и генерировать разнообразные вариации. VAEs широко используются для генерации контента с контролем латентных переменных.

## NeRF (Neural Radiance Fields)
подход к трёхмерной реконструкции сцены из множества изображений. NeRF обучает нейросеть, которая по координатам (x,y,z и углу обзора) предсказывает цвет и плотность точки 3D-пространства. Благодаря этому можно генерировать новые ракурсы сцены (например, анимировать поворот головы) на основе фотоснимков.

## ImageBind 
архитектура (от Meta), связывающая разные модальности (картинки, текст, аудио, сенсоры) в единое представление. Модель учится связывать информацию из разных источников, что облегчает мультимодальные задачи: например, понимание речи по губам.

## Segment Anything Model (SAM)
универсальная модель сегментации изображений (Meta), способная находить и выделять объекты без дополнительной тренировки. Её обучили на огромной коллекции аннотированных изображений, чтобы она могла «сегментировать что угодно» – от машин на дороге до повреждений на листе бумаги. В SAM достаточно «указать» точкой или фразой объект, а модель выдаёт маску.

## Visual Question Answering (VQA)
задачи, где модель отвечает на вопросы по изображению. Например, показывают картинку и спрашивают «Сколько людей на фото?» или «Какой цвет машины?», а мульти модальная модель должна понять картинку и текст запроса. VQA тестирует способность моделей совместно обрабатывать визуальную информацию и естественный язык.

## Vision-Language Pretraining (VLP)
стратегия совместного предобучения на парах «изображение–текст». Обычно модель обучается генерировать подписи к картинке или восстанавливать текст по изображению. Цель – создать общее представление, связывающее визуальные и языковые концепции. CLIP и DALL·E – примеры VLP-моделей.

## Multimodal Fusion (мультимодальное объединение)
объединение различных источников данных (видео, звук, текст, изображения) в одном модели. Например, при генерации подписи к видео учитывается как визуальная дорожка, так и звук. Это может быть простая конкатенация признаков или более сложные механизмы (cross-modal attention).

## Cross-modal Attention (кросс-модальный механизм внимания)
расширение self-attention, где внимание модель вычисляет между элементами разных модальностей. Например, в мультимодальном трансформере каждый «токен» изображения может обращать внимание на «токены» текста и наоборот. Это позволяет одному модулю «видеть» контекст другого.

## Visual Grounding (визуальное заякоривание)
задача связывания текста с элементами изображения. Например, если в описании сказано «человек слева красного цвета», модель должна указать именно этого человека на картинке. Это важно для понимания, какая часть изображения соответствует тексту.

## Captioning Models (модели генерации описаний)
модели, которые генерируют текстовые описания (подписи) к изображениям или видео. Например, DALL·E и Imagen могут не только создавать картинки, но и описывать, что изображено, поясняя предметы, действия, обстановку. Такие модели обучаются на парах «картинка–подпись».

## Multimodal Retrieval (мультимодальный поиск)
системы, которые по запросу из одной модальности (например, текст) ищут релевантный контент в другой (например, изображения). Например, запрос «красный автомобиль» должен вернуть фото машины. Или наоборот: по изображению найти связанные текстовые статьи. Работа основана на том, что модель строит общее представление картинок и текста.

## Embodied AI
направление, связанное с реальными роботами и агентами в физическом мире. «Воплощённый» ИИ включает восприятие через датчики и действия в среде. Например, робота-наставника учат не только отвечать на вопросы, но и передвигаться и манипулировать объектами. Этот подход объединяет компьютерное зрение, планирование, мультимодальные модели и управление.

# Аудио, видео и речь

## TTS (Text-to-Speech)
преобразование текста в речь. Модели TTS генерируют звуковой сигнал, интонируя и артикулируя текст. Современные системы (Tacotron, WaveNet, VITS) могут озвучивать текст почти без «роботности», используя глубокие нейросети и большие аудио-корпуса.

## ASR (Automatic Speech Recognition)
преобразование аудиоречи в текст. Модели ASR слушают голос и транскрибируют его. Включает шумоподавление, разделение речи/шёпота и даже понимание акцентов. Примеры: Whisper, DeepSpeech. Модели обучают на больших наборах «речь–текст» для разных языков.

## Voice Cloning (клонирование голоса)
адаптация TTS под конкретный голос. Достаточно короткой записи говорящего, чтобы модель могла генерировать речь, имитирующую его голос. Использует техники few-shot или fine-tuning: модель "узнаёт" уникальные тембр и интонации, сохраняя дикцию.

## Speech-to-Text (STT) 
синонимично ASR. Подчёркивает, что ключ – преобразовать произнесённые слова в письменные.

## Speech Enhancement (улучшение аудио)
технология улучшения качества звука: шумоподавление, устранение эхоподавления, восстановление искажений. Часто применяют нейросети для удаления фона или восстановления чистоты голоса (например, Deep Noise Suppression).

## Audio Captioning (аудио-подпись)
создание текстовых описаний к аудио-сигналам. Модель слушает звук (например, записи городской улицы) и генерирует описание: «Звуки сирен, гудки автомобилей и людской гул». Аналогично визуальному captioning, но для звука.

## Audio-Visual Fusion (аудиовизуальное объединение)
совместная обработка звука и изображения. Например, при синхронизации голоса и видео (липодаб) или для задач распознавания эмоций одновременно из речи и мимики. Мультимодальные модели могут понимать событийный контекст: видят, что человек жестикулирует, и слышат, что он говорит.

## Music Generation (музыкальная генерация)
создание музыки нейросетью. Примеры: MusicLM, Jukebox от OpenAI. Эти модели генерируют мелодии по текстовому или аудио-запросу, могут продолжать мелодию или создавать гармонии, используя огромные музыкальные датасеты.

## Lip Sync Models
модели синхронизации губ. Они берут аудио (речь) и генерируют движения губ персонажа или видеозапись говорящего. Часто используются в анимации и дипфейках: например, Dubbing-ты переводят персонажа на другой язык, сохраняя мимику.

## Emotion Recognition in Speech
распознавание эмоций по голосу. Модель анализирует тональность, интонацию, тембр, чтобы определить настроение говорящего (радость, гнев, грусть). Применяется в службах поддержки и психологии для анализа «как сказано», а не «что сказано».

# RL и генерация поведения

## Imitation Learning (обучение с подражанием)
обучение агента через наблюдение за экспертом. Вместо вознаграждения (как в RL) модель «копирует» действия демонстратора (человека или другого агента). Например, учить беспилотник летать, наблюдая видео полётов опытного пилота.

## Inverse Reinforcement Learning (обратное RL)
подход, где модель пытается вывести функцию вознаграждения эксперта по его поведению. Другими словами, по тому, как действовал опытный агент, система восстанавливает, что он пытался оптимизировать. Это полезно, когда трудно формализовать цели напрямую, но можно набрать примеры поведения «идеального» агента.

## Curriculum Learning (обучение по «учебной программе»)
метод, при котором задачи усложняются постепенно, как в обучении человека: сначала простые пробы, затем сложнее. Например, робот сначала учится ходить по прямой, затем по ухабистому. Это помогает легче достигать сложных навыков.

## Self-Play (самоигра)
агенты обучаются, играя сами с собой. В знаменитом AlphaZero, например, агент учился играть в шахматы, играя тысячи партий с собой и улучшая стратегию. Этот метод позволяет развивать сложные стратегии, даже без внешних примеров.

## World Models
идея, что агент создает внутреннюю «модель мира» (часто нейросеть), позволяющую симулировать среду и планировать в уме. Модель обучается предсказывать последствия действий (назад-прям) и затем использует эту симуляцию для обучения или планирования, ускоряя реальные опыты.

## Model-based RL
метод RL, где агент строит модель среды (прогноз переходов и наград) и планирует внутри неё. Это позволяет более эффективно учиться, используя предсказания.

## Model-free RL
классический подход RL, где агент прямо учится максимизировать награду без построения модели среды (например, Q-learning, Policy Gradients). Быстрее настраивается, но требует больше опытов, чем model-based.

## AlphaZero-style Training
метод итеративной самоигры с поиском (MCTS) и обучением нейросети. Агент с нуля учится сложным играм (шахматы, го) без заранее предоставленных данных: он играет сам с собой, улучшая стратегию через симуляции и дообучение нейросети на собственных партиях (по схеме RL + самоигра).

## Proximal Policy Optimization (PPO)
популярный алгоритм обучения с подкреплением. Он оптимизирует стратегию агента плавно, не позволяя слишком «оторваться» от текущей политики. Это повышает стабильность обучения в стохастических средах.

## Trust Region Policy Optimization (TRPO)
предшественник PPO: алгоритм RL, который гарантирует, что новая политика не слишком сильно отклоняется от старой (ограничивает «радиус доверия» изменений). Позволяет безопаснее обновлять стратегию, но сложнее вычислительно.

## Soft Actor-Critic (SAC)
RL-алгоритм, комбинирующий обучение с максимизацией энтропии (случайности) в действиях. Агент обучается выбирать не только наградо-оптимальные, но и «интересные» действия, что ускоряет обучение и делает его более устойчивым к переобучению.

# Безопасность и этика

## Constitutional AI (конституционный ИИ)
подход от Anthropic, где модель обучается следовать наборам «конституционных правил» (этических инструкций), заложенных человеком. Модель генерирует ответы, проверяемые другим «человеко-имитирующим» критериями. Это позволяет автоматически фильтровать нежелательные ответы без постоянного вмешательства человека.

## Alignment Tuning (настройка в соответствии с целями)
дообучение моделей с целью их «согласования» с человеческими ценностями и задачами. Включает RLHF, Constitutional AI, и другие техники, чтобы модель отвечала не просто корректно, но и полезно/этично/точно с человеческой точки зрения.

## Safety Layers (слои безопасности)
дополнительные модули или фильтры, которые обрабатывают вывод модели. Например, после генерации текст передаётся в «модуль токсичности», который блокирует неприемлемое. Это «экранирование» защиты работает наряду с самой моделью.

## Bias Mitigation (снятие смещений)
методы обнаружения и снижения предвзятости ИИ. Например, балансировка датасетов, дополнительные штрафы за некорректные ответы или адаптация моделей, чтобы они не воспроизводили стереотипы (по расе, полу, религии и т.д.).

## Explainable AI (объяснимый ИИ, XAI)
подходы, позволяющие понять, как модель пришла к своему выводу. Включает локальные объяснения (например, почему этот ответ), визуализацию внимания, правила принятия решений и т.д. Облегчает доверие и отладку моделей.

## Transparency Auditing (аудит прозрачности)
независимая проверка ИИ-систем на предмет их характеристик: откуда взяты данные, какие решения модель принимает, почему. Это нужно компаниям и регуляторам, чтобы удостовериться в соблюдении стандартов безопасности и честности.

## Differential Privacy (дифференциальная приватность)
техника приватного машинного обучения: данные обучающего набора шифруются или «шумятся» так, чтобы по выходу модели нельзя было восстановить информацию о конкретных записях. Позволяет учить модели на частных данных (медицинских или финансовых), сохраняя конфиденциальность.

## Federated Learning (федеративное обучение)
метод обучения распределённой модели на устройствах пользователей без передачи локальных данных. Устройство тренирует модель на своих данных, отправляет обновления (градиенты) в центр, где они агрегируются в глобальную модель. Это уменьшает риски утечки приватной информации.

# Метрики и оценка

## BERTScore
метрика качества текстового вывода, основанная на эмбеддингах BERT: оценивает семантическое сходство генерируемого текста и эталона. Чем ближе векторные представления, тем выше оценка. Лучше отражает смысл, чем точечное совпадение слов (BLEU).

## FactScore
метрика достоверности текста (задача fact-check). Она измеряет, насколько факты в ответе модели соотносятся с истинными данными. Часто включает подсистему извлечения фактов и их сравнение с базами знаний.

## TruthfulQA
тестовый набор вопросов, на которые модель часто отвечает неправильно, демонстрируя «галлюцинации» или предвзятость. Метрика оценивает процент правдивых ответов. Помогает проверять, насколько ИИ-ассистент выдаёт надежные факты.

## Winogrande
датасет для оценки рассуждений над общим знанием и языковой логикой. Содержит предложения с неоднозначными местоимениями и требует контекстуального понимания. Метрика – доля правильно разрешённых ситуаций (например, кем является «он»).

## HellaSwag
датасет ситуаций на практический «здравый смысл». Модель должна выбирать, как логично завершается сюжет. Оценка – точность выбора из нескольких вариантов. Используется для проверки общего рассуждения и языковой интуиции ИИ.

## GLEU, CHRF
метрики качества перевода и генерации. GLEU – модификация BLEU, более учитывающая порядок слов, CHRF – основана на совпадении n-грамм символов. Обе дают числовую оценку сходства машинного перевода с эталонным.
Consistency Metrics (метрики согласованности)
измеряют, насколько модель последовательна в разных ответах или при незначительных изменениях вопроса. Например, если спросить модель дважды схожий вопрос и получить разные ответы, коэффициент согласованности будет низким. Это важно для проверки стабильности и надёжности LLM.
